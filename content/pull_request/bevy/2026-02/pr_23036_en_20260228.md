+++
title = "#23036 Implement GPU clustering for lights, light probes, and decals"
date = "2026-02-28T00:00:00"
draft = false
template = "pull_request_page.html"
in_search_index = true

[taxonomies]
list_display = ["show"]

[extra]
current_language = "en"
available_languages = {"en" = { name = "English", url = "/pull_request/bevy/2026-02/pr-23036-en-20260228" }, "zh-cn" = { name = "中文", url = "/pull_request/bevy/2026-02/pr-23036-zh-cn-20260228" }}
labels = ["A-Rendering", "C-Performance", "M-Release-Note"]
+++

# Title: Implement GPU clustering for lights, light probes, and decals

## Basic Information
- **Title**: Implement GPU clustering for lights, light probes, and decals.
- **PR Link**: https://github.com/bevyengine/bevy/pull/23036
- **Author**: pcwalton
- **Status**: MERGED
- **Labels**: A-Rendering, C-Performance, S-Ready-For-Final-Review, M-Release-Note
- **Created**: 2026-02-18T06:30:21Z
- **Merged**: 2026-02-28T17:36:54Z
- **Merged By**: alice-i-cecile

## Description Translation

Currently, Bevy clusters lights on the CPU. This is generally not considered a best practice any longer, and it can be a bottleneck in workloads like `many_lights`. Moreover, it prevents GPU systems like [Hanabi] from creating clusterable objects such as lights and decals without a round trip to the CPU.

This PR introduces GPU light clustering when supported by the hardware. The algorithm is the same as the existing GPU light clustering, but parallelized over all clusters, and the resulting on-GPU format for clusters is unchanged. GPU light clustering uses the hardware rasterizer for compute purposes as a way to automatically distribute workloads within 2D axis-aligned bounding boxes without actually rendering any pixels, a first for Bevy. The algorithm is as follows, with each step corresponding to a raster or compute command:

1. *Z slicing*: We have a 3D cluster froxel grid of size WxHxD and seek to rasterize D axis-aligned quads, each of size WxH, representing the range of each clusterable object. In this compute phase, we generate D indirect instances for each clusterable object for the subsequent indirect draws.

2. *Count rasterization*: We use instanced indirect drawing to rasterize each quad generated in step 1 to a viewport of size WxH, with color writes disabled. Each rasterized fragment represents a cluster-object pair. In the fragment shader, we check to see if the object intersects the cluster, and, if it does, we atomically bump a counter corresponding to the number of objects of the given type intersecting the cluster in question. We don't record the ID of the object in this phase; we simply count the number of objects.

3. *Local allocation*: Now that we know the number of objects of each type in each cluster, we can proceed to allocate space in the clustered object buffer for each clustered object list. To do this, we need to perform a [*prefix sum*] operation so that each list is tightly packed with the others. For example, if adjacent clusters have 2, 5, and 3 objects, they'll be allocated at offsets 0, 2, and 7 respectively. This *local* step uses a [Hillis-Steele scan] in shared memory to compute the prefix sum of each chunk of 256 clusters. We can't go beyond 256 clusters in this local step because 256 is the maximum workgroup size in `wgpu`.

4. *Global allocation*: To deal with the fact that we can't calculate prefix sums beyond 256 clusters in step 3, we employ this second step that does a sequential loop over every 256-cluster chunk, propagating the prefix sum. At the end of this step, every list of clustered objects is allocated.

5. *Populate rasterization*: Finally, we issue an instanced indirect draw command using the same parameters as step (2). We test each cluster-object pair for intersection, and, if the test passes, we record the ID of each clustered object into the correct space in the list, using a scratch pad buffer of atomics to store the position of the next object in each list.

The buffer of clustered objects has a fixed size and can overflow. We detect this condition via asynchronous CPU readback and automatically grow the buffer for subsequent frames. In this case, we also log a warning so that the developer can choose a larger initial buffer size and avoid any incorrect frames. Additionally, like #22874, the automatic clustering heuristics are dynamically adjusted from frame to frame, by recording statistics on the GPU and using CPU readback to download them back to the CPU for processing.

As part of this PR, I refactored clustered visibility so that clustered objects go through the same `ViewVisibility` system as other objects, instead of using `VisibleClusterableObjects`. This was a nice simplification.

On the `many_lights` benchmark, with about 8,000 lights visible out of 100,000, this process takes approximately 0.099 ms on my NVIDIA GeForce RTX 4070 Laptop GPU. The AMD Ryzen 9 8945HS CPU, however, takes 2.12 ms to do the same task. The GPU version is therefore a 21x speedup.

`main` `assign_objects_to_clusters` time, 2.12 ms:
![CPU clustering time](https://github.com/user-attachments/assets/66341ad2-96f2-4e4a-87ee-fe3462bc05de)

GPU clustering GPU time, 0.099 ms:
![GPU clustering time](https://github.com/user-attachments/assets/18e2e0ae-a946-4b80-b38a-0543e76ebc02)

`main`, 5.71 ms median frame time, 175 FPS:
![CPU clustering frame time](https://github.com/user-attachments/assets/111c8e22-414f-4ee1-95fa-d7cfe422c2ab)

GPU clustering, 4.88 ms median frame time, 205 FPS:
![GPU clustering frame time](https://github.com/user-attachments/assets/0a662e88-a1b9-49c8-8bab-cc12b46cd079)

[Hanabi]: https://github.com/djeedai/bevy_hanabi

[*prefix sum*]: https://en.wikipedia.org/wiki/Prefix_sum

[Hillis-Steele scan]: https://en.wikipedia.org/wiki/Prefix_sum#Algorithm_1:_Shorter_span,_more_parallel

## Alice's PM Note from @kfc35

Fixes https://github.com/bevyengine/bevy/issues/22957 and also fixes https://github.com/bevyengine/bevy/issues/22904.

## The Story of This Pull Request

### The Problem and Context

Bevy's rendering engine uses clustered forward rendering to efficiently determine which lights, light probes, and decals affect each region (cluster) of the screen. Before this PR, Bevy performed this clustering entirely on the CPU. While functional, this approach had several significant limitations:

1. **Performance bottleneck**: CPU-based clustering was a measurable performance cost, particularly in scenes with many lights (like the `many_lights` benchmark).
2. **Architectural constraint**: Systems that run entirely on the GPU (like particle effects from the Hanabi library) couldn't create lights or decals that participate in clustering without expensive CPU round trips.
3. **Algorithmic complexity**: The CPU algorithm used iterative sphere refinement, which is complex and inherently sequential, making it difficult to parallelize effectively.

The CPU clustering implementation was also architecturally complex, using a separate `VisibleClusterableObjects` component to track which clusterable objects were visible to each view.

### The Solution Approach

The developer implemented GPU-based clustering using a five-step algorithm that leverages the hardware rasterizer as a compute mechanism. This approach is novel for Bevy - using the rasterizer not to draw pixels, but to efficiently schedule compute work across 2D axis-aligned bounding boxes.

The solution includes several key engineering decisions:

1. **Hardware compatibility**: The system automatically detects whether GPU clustering is supported (requires storage buffers and compute shaders) and falls back to CPU clustering when necessary.
2. **Buffer management**: Dynamic buffer resizing with asynchronous CPU readback handles overflow conditions gracefully.
3. **Architecture simplification**: The PR refactors the clustered visibility system to use the existing `ViewVisibility` system instead of the custom `VisibleClusterableObjects` component.
4. **Algorithm design**: The five-step algorithm (Z slicing, count rasterization, local allocation, global allocation, populate rasterization) balances parallelism with workgroup size limitations.

### The Implementation

The implementation adds approximately 2,600 lines of new code across 19 files, with the core logic in:

1. **`gpu.rs`**: Contains the main GPU clustering implementation with the five-step algorithm.
2. **Three new WGSL shaders**: Handle Z slicing, rasterization, and allocation.
3. **Modified CPU clustering**: Now conditionally runs only when GPU clustering is disabled.

The GPU clustering system integrates with Bevy's existing rendering architecture by:

1. **Extraction phase**: When GPU clustering is enabled, the system extracts cluster configurations but doesn't run CPU clustering.
2. **Preparation phase**: Creates and uploads necessary buffers (Z slices, metadata, scratchpad).
3. **Rendering phase**: Executes the five-step algorithm via compute and raster passes.
4. **Readback phase**: Asynchronously reads statistics back to CPU for dynamic adjustment.

A key insight is how the rasterizer is used for compute: by setting up a viewport matching the cluster grid dimensions (W×H) and drawing axis-aligned quads for each Z slice, each fragment shader invocation naturally corresponds to a specific cluster-object pair. Color writes are disabled since we're only using the rasterizer for fragment scheduling.

### Technical Insights

**Prefix Sum Implementation**: The allocation phase requires a prefix sum to pack cluster lists contiguously. Due to wgpu's 256 workgroup size limit, this uses a two-pass approach:
- Local pass: Hillis-Steele scan over 256-cluster chunks
- Global pass: Sequential propagation across chunks

**Buffer Overflow Handling**: The system detects when buffers are too small via GPU→CPU readback and automatically resizes them, logging warnings to guide developers in choosing appropriate initial sizes.

**Simplified Visibility**: By using `ViewVisibility` instead of `VisibleClusterableObjects`, clusterable objects now participate in Bevy's standard visibility system. This removes custom code paths and makes the system more consistent.

**Performance Characteristics**: The implementation achieves a 21× speedup (0.099 ms vs 2.12 ms) on the `many_lights` benchmark. This improvement comes from both algorithmic changes (parallel vs sequential) and hardware utilization (GPU compute units vs CPU cores).

### The Impact

**Performance**: Significant reduction in clustering time (21× faster) translates to measurable frame time improvements (5.71 ms → 4.88 ms median frame time in the benchmark).

**Architectural improvements**: 
- Removed the `VisibleClusterableObjects` component and associated complexity
- Unified clusterable object visibility with the standard `ViewVisibility` system
- Enabled GPU systems to create clusterable objects without CPU intervention

**Feature enablement**: GPU systems like Hanabi can now create lights and decals that participate in clustering directly on the GPU.

**Developer experience**: Automatic buffer resizing with clear warning messages helps developers understand when to adjust buffer capacities.

The implementation maintains backward compatibility by automatically detecting hardware support and falling back to CPU clustering when GPU clustering isn't available. This ensures the feature works across Bevy's supported platforms while providing optimal performance on capable hardware.

## Visual Representation

```mermaid
graph TD
    A[Clusterable Objects<br/>(Lights, Probes, Decals)] --> B[Z Slicing Compute Pass]
    B --> C[Count Rasterization Pass]
    C --> D[Local Allocation Compute Pass]
    D --> E[Global Allocation Compute Pass]
    E --> F[Populate Rasterization Pass]
    F --> G[Clustered Index Lists]
    
    H[View Configuration] --> I[Cluster Grid Setup]
    I --> C
    I --> F
    
    J[Statistics Readback] --> K[Dynamic Buffer Resizing]
    K --> B
    K --> D
```

## Key Files Changed

### `crates/bevy_pbr/src/cluster/gpu.rs` (+1714/-0)
**Purpose**: Implements the core GPU clustering algorithm and plugin.

**Key changes**:
- New `GpuClusteringPlugin` that conditionally enables GPU clustering
- Five-step algorithm implementation (z_slice_main, allocate_local_main, allocate_global_main, vertex_main, fragment_main)
- Buffer management with automatic resizing via CPU readback
- Integration with Bevy's rendering systems

**Code snippet showing the main clustering function**:
```rust
/// The render command building system that performs GPU clustering on each
/// view.
fn cluster_on_gpu(
    view_query: ViewQuery<(
        &MainEntity,
        Option<&ViewGpuClusteringBuffers>,
        Option<&ViewGpuClusteringPipelineIds>,
        Option<&ViewClusteringDummyTexture>,
        Option<&ViewClusteringBindGroups>,
        Option<&ViewLightProbesUniformOffset>,
        Option<&ViewLightsUniformOffset>,
        Option<&ViewUniformOffset>,
        Option<&ExtractedClusterConfig>,
    )>,
    pipeline_cache: Res<PipelineCache>,
    clustering_mesh_buffers: Res<GpuClusteringMeshBuffers>,
    render_view_clustering_readback_data: Res<RenderViewClusteringReadbackData>,
    mut render_context: RenderContext,
) {
    // ... setup and validation
    
    // Pass 1: Z slicing.
    run_clustering_z_slicing_pass(...);
    
    // Pass 2: Count raster.
    run_clustering_rasterization_pass(..., false);
    
    // Pass 3: local allocation.
    run_clustering_allocation_pass(..., false);
    
    // Pass 4: global allocation.
    run_clustering_allocation_pass(..., true);
    
    // Pass 5: populate raster.
    run_clustering_rasterization_pass(..., true);
    
    // Schedule readback for statistics
    schedule_readback_staging(...);
    schedule_readback_buffer_map(...);
}
```

### `crates/bevy_light/src/cluster/assign.rs` (+463/-485)
**Purpose**: Modified CPU clustering to conditionally run only when GPU clustering is disabled.

**Key changes**:
- Added check for `global_cluster_settings.gpu_clustering.is_none()` before running CPU clustering
- Removed `VisibleClusterableObjects` usage in favor of `ViewVisibility`
- Simplified query structures to use `ViewVisibility` component

**Code snippet showing the conditional logic**:
```rust
// Collect clusterable objects if GPU clustering is enabled.
if global_cluster_settings.gpu_clustering.is_none() {
    // ... CPU clustering logic
}
```

### `crates/bevy_pbr/src/cluster/cluster_raster.wgsl` (+525/-0)
**Purpose**: Shader for count and populate rasterization passes.

**Key changes**:
- Fragment shader tests sphere-AABB intersection for cluster-object pairs
- Two versions: count pass (atomic increments) and populate pass (writes indices)
- Includes spot light cone culling logic

**Code snippet showing intersection test**:
```wgsl
// See if the object sphere intersects the AABB. If it doesn't, cull the
// object.
let object_intersects_cluster_aabb = sphere_intersects_aabb(
    varyings.sphere_position,
    varyings.sphere_radius,
    cluster_aabb_center,
    cluster_aabb_half_size
);
if (!object_intersects_cluster_aabb) {
    return vec4<f32>(0.0);
}
```

### `crates/bevy_pbr/src/cluster/mod.rs` (+173/-46)
**Purpose**: Updated cluster module to support both CPU and GPU clustering.

**Key changes**:
- Added `ClusterableObjects` enum to track whether clustering is CPU or GPU
- Modified `Clusters` struct to use `ClusterableObjects` instead of direct `Vec`
- Added GPU clustering constants and helper functions

**Code snippet showing the new enum**:
```rust
/// The list of objects within a cluster, if known to the CPU.
#[derive(Debug)]
pub enum ClusterableObjects {
    /// The list of objects in the cluster is known to the CPU.
    Cpu(Vec<ObjectsInClusterCpu>),
    /// The list of objects in the cluster is unknown to the CPU, because GPU
    /// clustering is being used.
    Gpu,
}
```

### `crates/bevy_pbr/src/cluster/cluster_z_slice.wgsl` (+203/-0)
**Purpose**: Compute shader for Z slicing pass (step 1).

**Key changes**:
- Computes Z slice range for each clusterable object
- Writes Z slice data to buffer for rasterization passes
- Calculates farthest Z value for dynamic cluster range adjustment

**Code snippet showing Z slice calculation**:
```wgsl
// Write out our Z slices.
for (var z_slice = cluster_bounds.min.z; z_slice <= cluster_bounds.max.z; z_slice += 1u) {
    try_write_z_slice(object_index, object_type, z_slice);
}
```

## Further Reading

1. **Clustered Shading Algorithms**:
   - [Practical Clustered Shading](http://newq.net/dl/pub/s2015_practical.pdf) - The original paper on clustered shading
   - [Clustered Deferred and Forward Shading](https://software.intel.com/content/www/us/en/develop/articles/forward-clustered-shading.html) - Intel's implementation details

2. **Parallel Algorithms**:
   - [Prefix Sum (Scan) Algorithms](https://en.wikipedia.org/wiki/Prefix_sum) - Foundation for the allocation phase
   - [Hillis-Steele Scan](https://en.wikipedia.org/wiki/Prefix_sum#Algorithm_1:_Shorter_span,_more_parallel) - Parallel prefix sum algorithm used

3. **Bevy Documentation**:
   - [Bevy's Rendering Architecture](https://bevyengine.org/learn/book/rendering/) - Understanding Bevy's render graph
   - [WGSL Shader Language](https://bevyengine.org/learn/book/rendering/shaders/) - Bevy's shading language

4. **GPU Compute Patterns**:
   - [GPU-Based Compute Using Rasterization](https://developer.nvidia.com/gpugems/gpugems2/part-iv-general-purpose-computation-gpus-primer/chapter-32-taking-plunge-gpu) - Using rasterizers for general-purpose compute
   - [Atomic Operations in GPU Shaders](https://www.khronos.org/opengl/wiki/Memory_Model#Atomic_operations) - Understanding atomic operations used in the counting phase