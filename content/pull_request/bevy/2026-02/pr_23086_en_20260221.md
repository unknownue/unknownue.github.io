+++
title = "#23086 Fix buffered channel memory leak"
date = "2026-02-21T00:00:00"
draft = false
template = "pull_request_page.html"
in_search_index = true

[taxonomies]
list_display = ["show"]

[extra]
current_language = "en"
available_languages = {"en" = { name = "English", url = "/pull_request/bevy/2026-02/pr-23086-en-20260221" }, "zh-cn" = { name = "中文", url = "/pull_request/bevy/2026-02/pr-23086-zh-cn-20260221" }}
labels = ["C-Bug", "P-Regression", "A-Transform", "A-Tasks"]
+++

# Fix buffered channel memory leak

## Basic Information
- **Title**: Fix buffered channel memory leak
- **PR Link**: https://github.com/bevyengine/bevy/pull/23086
- **Author**: aevyrie
- **Status**: MERGED
- **Labels**: C-Bug, S-Ready-For-Final-Review, P-Regression, A-Transform, A-Tasks
- **Created**: 2026-02-20T23:05:09Z
- **Merged**: 2026-02-21T11:08:19Z
- **Merged By**: mockersf

## Description Translation
This PR description is already in English, so no translation is needed:

# Objective

- Fix a possible memory leak when a producer and consumer are on different threads and never switch.

## Solution

- Set a maximum size for the object pool to avoid unbounded growth.

## Testing

- Reproduced leak in `bevy_city` with moving cars.
- Verified leak is no longer present with this patch.

Closes #23038 
Introduced in #22297

## The Story of This Pull Request

The issue began with a regression introduced in PR #22297, where the buffered channel implementation in Bevy's task system was optimized to use a pool of reusable vectors for message passing. While this optimization reduced allocation overhead, it created a subtle edge case that could lead to unbounded memory growth under specific thread scheduling conditions.

At the core of the problem was the behavior of `RecycledVec`, a wrapper type that automatically returns message buffers to the channel's pool when dropped. The original implementation unconditionally pushed every cleared vector back into a thread-local pool. This worked well when producer and consumer threads alternated or when the system had balanced communication patterns. However, when a producer thread continuously sent messages to a consumer on a different thread without the consumer ever running on the producer's thread, the thread-local pool would accumulate vectors without bound.

Consider a scenario where Thread A produces messages and Thread B consumes them. Each time Thread A sends a message, it allocates a vector or reuses one from its thread-local pool. When the consumer on Thread B processes the message and drops the `RecycledVec`, the vector gets pushed into Thread B's thread-local pool. The problem arises because Thread A never executes on Thread B, so it never has the opportunity to reclaim those vectors. This creates a one-way flow of vectors from Thread A's allocation pool to Thread B's retention pool, leading to linear memory growth proportional to the number of messages sent.

The fix introduces two key changes. First, it adds a maximum size limit to the object pool with a constant `MAX_POOL_SIZE` set to 8. This prevents unbounded growth by discarding excess vectors rather than storing them indefinitely. Second, it centralizes the recycling logic into a new `recycle` method on `BufferedChannel`, ensuring consistent behavior across both the `RecycledVec` drop implementation and the `BufferedSender::flush` method.

Here's the core of the fix:

```rust
impl<T: Send> BufferedChannel<T> {
    const MAX_POOL_SIZE: usize = 8;

    fn recycle(&self, mut chunk: Vec<T>) {
        if chunk.capacity() < self.chunk_size {
            return;
        }
        chunk.clear();
        let mut pool = self.pool.borrow_local_mut();
        if pool.len() < Self::MAX_POOL_SIZE {
            // Only push to the pool if it's not full
            // Avoids memory leak if the sender and receiver never switch threads
            pool.push(chunk);
        }
    }
}
```

The method checks two conditions before recycling a vector: it must have sufficient capacity (at least `self.chunk_size`), and the pool must have available slots below the maximum size. Vectors that don't meet these criteria are simply dropped, allowing the Rust allocator to reclaim their memory.

The fix also changes the `Drop` implementation for `RecycledVec` to use this centralized recycling logic:

```rust
impl<'a, T: Send> Drop for RecycledVec<'a, T> {
    fn drop(&mut self) {
        if let Some(buffer) = self.buffer.take() {
            self.channel.recycle(buffer);
        }
    }
}
```

Additionally, the `BufferedSender::flush` method was updated to use the same recycling logic and was made public. This change allows external code to explicitly flush buffered messages, which could be useful for scenarios where you want to ensure messages are sent without waiting for the sender to be dropped.

The choice of `MAX_POOL_SIZE = 8` represents a reasonable trade-off between memory efficiency and allocation performance. A pool of 8 vectors provides good reuse opportunities for bursty message patterns while preventing excessive memory retention. The value is small enough that the memory overhead is negligible even in worst-case scenarios where every thread retains its maximum pool size.

The fix was validated by reproducing the leak in the `bevy_city` example with moving cars, where the continuous spawning and despawning of entities could trigger the pathological thread behavior. After applying the patch, memory usage stabilized, confirming that the leak was resolved.

This issue highlights an important consideration in concurrent programming: object pools that rely on thread-local storage must account for the possibility of unbalanced thread workloads. Without proper bounds checking, such pools can transform performance optimizations into memory leaks. The solution demonstrates a common pattern in Rust systems programming—using RAII (Resource Acquisition Is Initialization) with careful drop implementations to manage resources, combined with explicit bounds checking to prevent resource exhaustion.

## Visual Representation

```mermaid
graph TD
    A[BufferedChannel] --> B[Thread-local Pool]
    B --> C{Is pool full?}
    C -->|No| D[Store vector in pool]
    C -->|Yes| E[Drop vector]
    
    F[Producer Thread] --> G[Send message]
    G --> H[Allocate/Reuse vector]
    H --> I[Consumer Thread processes]
    I --> J[RecycledVec dropped]
    J --> K[Call channel.recycle()]
    K --> C
```

## Key Files Changed

### `crates/bevy_utils/src/buffered_channel.rs` (+21/-5)

This file contains the buffered channel implementation used for task communication in Bevy's ECS (Entity Component System). The changes fix a memory leak in the object pool used to recycle message vectors.

**Key changes:**
1. Added a `MAX_POOL_SIZE` constant and `recycle` method to `BufferedChannel`
2. Updated `RecycledVec` drop implementation to use centralized recycling
3. Updated `BufferedSender::flush` to use the same recycling logic and made it public

**Code snippets:**

Before the fix, vectors were unconditionally pushed to the pool:
```rust
impl<'a, T: Send> Drop for RecycledVec<'a, T> {
    fn drop(&mut self) {
        if let Some(mut buffer) = self.buffer.take() {
            buffer.clear();
            self.channel.pool.borrow_local_mut().push(buffer);
        }
    }
}
```

After the fix, recycling is bounded:
```rust
impl<T: Send> BufferedChannel<T> {
    const MAX_POOL_SIZE: usize = 8;

    fn recycle(&self, mut chunk: Vec<T>) {
        if chunk.capacity() < self.chunk_size {
            return;
        }
        chunk.clear();
        let mut pool = self.pool.borrow_local_mut();
        if pool.len() < Self::MAX_POOL_SIZE {
            pool.push(chunk);
        }
    }
}

impl<'a, T: Send> Drop for RecycledVec<'a, T> {
    fn drop(&mut self) {
        if let Some(buffer) = self.buffer.take() {
            self.channel.recycle(buffer);
        }
    }
}
```

The `flush` method was also updated:
```rust
// Before:
fn flush(&mut self) {
    if let Some(buffer) = self.buffer.take() {
        if !buffer.is_empty() {
            let _ = bevy_platform::future::block_on(self.tx.send(buffer));
        } else {
            self.channel.pool.borrow_local_mut().push(buffer);
        }
    }
}

// After:
pub fn flush(&mut self) {
    if let Some(buffer) = self.buffer.take() {
        if !buffer.is_empty() {
            let _ = bevy_platform::future::block_on(self.tx.send(buffer));
        } else {
            self.channel.recycle(buffer);
        }
    }
}
```

## Further Reading

1. **Rust RAII Pattern**: The `Drop` trait implementation in this PR follows the RAII pattern common in Rust. Understanding this pattern is essential for resource management in Rust applications.

2. **Thread-Local Storage in Rust**: The fix addresses a specific issue with thread-local object pools. The Rust standard library's `thread_local!` macro and `LocalKey` type are relevant for understanding the context.

3. **Bounded Object Pools**: The concept of bounded pools is a common solution to prevent resource exhaustion in concurrent systems. The "Leaky Bucket" algorithm and other rate-limiting patterns share similar principles.

4. **Bevy Task System**: Understanding how Bevy's ECS uses channels for task communication provides context for why this bug was significant. The Bevy documentation on tasks and async systems would be helpful.

5. **Memory Leak Detection in Rust**: While Rust prevents many memory safety issues, logical memory leaks (like this one) can still occur. Tools like Valgrind, heaptrack, or Rust's built-in allocation tracking can help identify such issues.

6. **PR #22297**: Reviewing the original PR that introduced the regression provides insight into the optimization that inadvertently caused the leak.