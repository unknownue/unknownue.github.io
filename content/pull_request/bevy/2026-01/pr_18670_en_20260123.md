+++
title = "#18670 Remote entity reservation v9"
date = "2026-01-23T00:00:00"
draft = false
template = "pull_request_page.html"
in_search_index = true

[taxonomies]
list_display = ["show"]

[extra]
current_language = "en"
available_languages = {"en" = { name = "English", url = "/pull_request/bevy/2026-01/pr-18670-en-20260123" }, "zh-cn" = { name = "中文", url = "/pull_request/bevy/2026-01/pr-18670-zh-cn-20260123" }}
labels = ["C-Feature", "A-ECS", "M-Migration-Guide", "X-Controversial", "S-Needs-SME"]
+++

# Title

## Basic Information
- **Title**: Remote entity reservation v9
- **PR Link**: https://github.com/bevyengine/bevy/pull/18670
- **Author**: ElliottjPierce
- **Status**: MERGED
- **Labels**: C-Feature, A-ECS, M-Migration-Guide, X-Controversial, S-Needs-SME
- **Created**: 2025-04-01T19:41:01Z
- **Merged**: 2026-01-23T02:53:26Z
- **Merged By**: cart

## Description Translation

fixes #18003

# Objective

It's version 9 of the same objective lol. For assets as entities, we need entities to be able to be reserved from any thread. Ideally, this can be done without depending on an async context, blocking, or waiting. Any of these compromises *could* hurt asset performance or discontinue the completely non-blocking nature of the asset system. 

As a bonus, this PR makes allocating entities only need `&Entities` instead of `&mut`. `Entities::flush` is now completely optional, meaning none of the `Entities` methods depends on the flush at all, and there is protection against flushing an entity twice.

(If you're curious, v9 actually branched from v8. v8 was focused on #18577 (never flush entities), but this still includes flushing.)

> There's a doc [here](https://hackmd.io/@bevy/Bkjgqvcblx) that builds some background for this too. If you haven't been following this I'd highly recommend reading that before diving into the code.

## Solution

Organizationally, I split off the underlying `EntityAllocator` from `Entities`. This makes it easier to read, etc, now that it's more involved.

The basic problem is that we need to be able to allocate an entity from any thread at any time. We also need to be able to free an entity. So at the allocator level, that's 3 operations: `free`, `alloc` (For when you know `free` isn't being called), and `remote_alloc` (can be called any time). None of these can require mutable access.

The biggest challenge is having a list of entities that are `free` and waiting to be re-used. The list needs to be fully functional without mutable access, needs to be resizable, and needs to be pinned in memory. I ended up using a strategy similar to [`SplitVec`](https://docs.rs/orx-split-vec/latest/orx_split_vec/). That dependency requires `std`, and knowing the max capacity ahead of time lets us simplify the implementation, so I made my own implementation here.

## Testing

No new tests right now. It might be worth using [loom](https://docs.rs/loom/latest/loom/) at some point, but that requires an additional dependency, test-specific loom feature flags, and giving this treatment to multiple crates, especially bevy_platform. 

## Future work

#18577 is still a good end game here IMO. Ultimately, (just like @maniwani said would happen), I decided that doing this all at once would be both too challenging and add too much complexity. However, v9 makes "never flush" much, *much* more approachable for the future. The biggest issues I ran into were that lots of places hold a reference to an entity's `Archetype` (but the entity now might not have an archetype) and that checking archetypes everywhere *might* actually be less performant than flushing. Maybe.

We can also potentially speed up a lot of different processes now that `alloc` can be called without mutable access and `free` (etc.) can be called without needing to `flush` first.

## Costs

<details>
  <summary>Benchmarks</summary>

```txt
group                                           main_baseline                           remote_reservation_v9_baseline
-----                                           -------------                           ------------------------------
add_remove/sparse_set                           1.06   625.2±42.08µs        ? ?/sec     1.00   591.8±24.97µs        ? ?/sec
add_remove/table                                1.00   883.9±66.56µs        ? ?/sec     1.06   941.1±34.62µs        ? ?/sec
add_remove_very_big/table                       1.08     37.7±2.55ms        ? ?/sec     1.00     34.9±0.76ms        ? ?/sec
added_archetypes/archetype_count/1000           1.13  688.8±175.58µs        ? ?/sec     1.00  608.6±137.61µs        ? ?/sec
added_archetypes/archetype_count/200            1.13    72.7±18.97µs        ? ?/sec     1.00    64.2±20.78µs        ? ?/sec
added_archetypes/archetype_count/2000           1.11  1086.7±290.33µs        ? ?/sec    1.00  977.2±118.15µs        ? ?/sec
added_archetypes/archetype_count/5000           1.09      2.7±0.29ms        ? ?/sec     1.00      2.5±0.27ms        ? ?/sec
despawn_world/10_entities                       1.00   695.6±13.85ns        ? ?/sec     1.09   760.4±40.01ns        ? ?/sec
despawn_world/1_entities                        1.00   182.0±24.14ns        ? ?/sec     1.56   284.3±50.46ns        ? ?/sec
despawn_world_recursive/10000_entities          1.00  1668.8±95.30µs        ? ?/sec     1.13  1878.0±111.75µs        ? ?/sec
despawn_world_recursive/10_entities             1.00      2.3±0.04µs        ? ?/sec     1.05      2.4±0.09µs        ? ?/sec
despawn_world_recursive/1_entities              1.00   382.2±36.07ns        ? ?/sec     1.41   539.1±60.46ns        ? ?/sec
empty_archetypes/iter/10000                     1.07     12.8±1.61µs        ? ?/sec     1.00     12.0±0.49µs        ? ?/sec
empty_archetypes/par_for_each/100               1.06      9.2±1.04µs        ? ?/sec     1.00      8.7±0.35µs        ? ?/sec
empty_archetypes/par_for_each/1000              1.12     12.8±0.87µs        ? ?/sec     1.00     11.5±0.36µs        ? ?/sec
empty_archetypes/par_for_each/10000             1.19     25.4±0.98µs        ? ?/sec     1.00     21.3±0.41µs        ? ?/sec
empty_archetypes/par_for_each/2000              1.16     13.6±1.17µs        ? ?/sec     1.00     11.7±0.44µs        ? ?/sec
empty_archetypes/par_for_each/500               1.08     11.1±0.70µs        ? ?/sec     1.00     10.3±0.28µs        ? ?/sec
empty_commands/0_entities                       1.00      3.9±0.06ns        ? ?/sec     1.40      5.4±0.06ns        ? ?/sec
entity_hash/entity_set_lookup_miss_gen/10000    1.00     41.6±6.26µs 229.0 MElem/sec    1.05     43.9±5.74µs 217.1 MElem/sec
entity_hash/entity_set_lookup_miss_id/10000     1.25     44.5±5.30µs 214.2 MElem/sec    1.00     35.7±5.03µs 266.8 MElem/sec
event_propagation/four_event_types              1.13   606.5±27.06µs        ? ?/sec     1.00    535.9±5.38µs        ? ?/sec
event_propagation/single_event_type             1.12   870.3±27.20µs        ? ?/sec     1.00   776.4±18.86µs        ? ?/sec
fake_commands/2000_commands                     1.00     12.1±0.08µs        ? ?/sec     1.28     15.4±0.25µs        ? ?/sec
fake_commands/4000_commands                     1.00     24.2±0.26µs        ? ?/sec     1.28     30.9±0.38µs        ? ?/sec
fake_commands/6000_commands                     1.00     36.3±0.50µs        ? ?/sec     1.27     46.2±0.48µs        ? ?/sec
fake_commands/8000_commands                     1.00     48.3±0.15µs        ? ?/sec     1.28     61.6±0.87µs        ? ?/sec
insert_simple/base                              1.29   403.9±79.33µs        ? ?/sec     1.00   312.1±56.57µs        ? ?/sec
insert_simple/unbatched                         2.41  1021.5±234.06µs        ? ?/sec    1.00   423.2±17.00µs        ? ?/sec
iter_fragmented/base                            1.00    346.6±8.58ns        ? ?/sec     1.40    485.0±9.30ns        ? ?/sec
iter_fragmented/foreach                         1.07    141.4±6.76ns        ? ?/sec     1.00    132.4±3.85ns        ? ?/sec
iter_fragmented_sparse/base                     1.19      7.9±0.16ns        ? ?/sec     1.00      6.6±0.09ns        ? ?/sec
iter_simple/foreach_wide                        2.76     46.4±0.46µs        ? ?/sec     1.00     16.8±0.18µs        ? ?/sec
query_get/50000_entities_table                  1.00    138.8±0.67µs        ? ?/sec     1.05    145.9±1.23µs        ? ?/sec
query_get_many_5/50000_calls_sparse             1.06   607.3±14.47µs        ? ?/sec     1.00   570.5±42.57µs        ? ?/sec
sized_commands_0_bytes/2000_commands            1.00     10.6±1.20µs        ? ?/sec     1.26     13.3±0.18µs        ? ?/sec
sized_commands_0_bytes/4000_commands            1.00     20.6±0.33µs        ? ?/sec     1.29     26.5±0.42µs        ? ?/sec
sized_commands_0_bytes/6000_commands            1.00     30.8±0.26µs        ? ?/sec     1.29     39.9±0.61µs        ? ?/sec
sized_commands_0_bytes/8000_commands            1.00     41.4±0.77µs        ? ?/sec     1.28     53.1±0.65µs        ? ?/sec
sized_commands_12_bytes/2000_commands           1.00     11.6±0.29µs        ? ?/sec     1.23     14.2±0.32µs        ? ?/sec
sized_commands_12_bytes/4000_commands           1.00     22.8±3.30µs        ? ?/sec     1.24     28.3±0.30µs        ? ?/sec
sized_commands_12_bytes/6000_commands           1.00     33.6±0.20µs        ? ?/sec     1.27     42.8±0.63µs        ? ?/sec
sized_commands_12_bytes/8000_commands           1.00     48.8±6.10µs        ? ?/sec     1.22     59.7±0.76µs        ? ?/sec
sized_commands_512_bytes/2000_commands          1.00     46.3±1.28µs        ? ?/sec     1.06     48.9±1.73µs        ? ?/sec
sized_commands_512_bytes/4000_commands          1.00     90.5±2.06µs        ? ?/sec     1.08     97.4±3.33µs        ? ?/sec
spawn_commands/2000_entities                    1.00   155.4±11.61µs        ? ?/sec     1.22   189.5±15.70µs        ? ?/sec
spawn_commands/4000_entities                    1.00   303.7±15.69µs        ? ?/sec     1.22   371.1±18.58µs        ? ?/sec
spawn_commands/6000_entities                    1.00   463.3±31.95µs        ? ?/sec     1.20   554.8±12.55µs        ? ?/sec
spawn_commands/8000_entities                    1.00   619.7±44.57µs        ? ?/sec     1.19   734.9±16.71µs        ? ?/sec
spawn_world/1000_entities                       1.06     41.5±2.97µs        ? ?/sec     1.00     39.1±2.94µs        ? ?/sec
spawn_world/100_entities                        1.20      4.8±2.29µs        ? ?/sec     1.00      4.0±0.69µs        ? ?/sec
spawn_world/10_entities                         1.15   461.9±80.36ns        ? ?/sec     1.00   400.5±26.17ns        ? ?/sec
spawn_world/1_entities                          1.05     41.4±6.35ns        ? ?/sec     1.00     39.4±3.02ns        ? ?/sec
world_get/50000_entities_sparse                 1.00    167.1±4.23µs        ? ?/sec     1.07    178.8±2.44µs        ? ?/sec
world_query_get/50000_entities_sparse_wide      1.00    125.0±0.31µs        ? ?/sec     1.08   135.7±78.30µs        ? ?/sec
world_query_iter/50000_entities_sparse          1.00     38.7±0.08µs        ? ?/sec     1.18     45.6±0.60µs        ? ?/sec
```
</details>

**Interpreting benches:**

In most places, v9 is on par with or even faster than main. Some notable exceptions are the "sized_commands" and "fake_commands" sections, but the regression there is purely due to `Entities::flush` being slower, but we make up for that elsewhere. These commands don't actually do anything though, so this is not relevant to actual use cases. The benchmarks just exist to stress test `CommandQueue`.

The only place where v9 causes a significant and real-world applicable regression is "spawn_commands", where v9 is roughly 15% slower than main. This is something that can be changed later now that `alloc` doesn't need mutable access. I expect we can change this 15% regression to a 15% improvement given that "spawn_world" is roughly 20% faster on v9 than on main. For users that need really fast spawn commands though, they are already using some form of batch spawning or direct world access.

Other regressions seem to be either minimal, unrealistic, easily corrected in the future, or wrong. I feel confident saying "wrong" since running them back to back can sometimes yield different results. I'm on a M2 Max, so there might be some things jumping from perf cores to efficiency cores or something. (I look forward to standardized benchmarking hardware.)

**Wins:** I was worried that, without "never flush", this would be an overall regression, but I am relived that that is not the case. Some very common operations, "insert_simple/unbatched" for example, are way faster on this branch than on main. Basically, on main, `alloc` also adds `EntityMeta` for the entity immediately, but on this branch, we only do so in `set`. That seems to improve temporal cache locality and leads to this roughly 220% improvement. "added_arhcetype" sees 20%-80% improvements too, etc. "iter_simple/foreach_wide" also sees a 270% improvement.

I think in practice, v9 will out perform main for real-world schedules. And I think moving towards "never flush" (even for only a few operations, like `Commands::spawn`) will improve performance even more.

## The Story of This Pull Request

This PR addresses a fundamental problem in Bevy's ECS system: the need to allocate entities from any thread without blocking, while maintaining the performance characteristics necessary for a game engine. The challenge stems from Bevy's move toward treating assets as entities, which requires non-blocking entity allocation during async operations like asset loading.

The existing entity allocation system had several limitations. First, entity allocation required mutable access to the `Entities` resource, which prevented concurrent allocation from multiple threads. Second, the system relied on periodic "flushing" operations to make allocated entities usable in queries, which added complexity and potential performance overhead. Third, there was no safe way to allocate entities from remote threads or async contexts without potentially compromising the non-blocking nature of the system.

The solution takes a comprehensive approach by completely redesigning the entity allocator's internals. At the core, the PR separates the allocation logic from the `Entities` structure, creating a new `remote_allocator` module that contains the allocation machinery. This separation makes the code more maintainable and allows for different allocation strategies.

The key insight behind the implementation is that entity allocation involves three distinct operations with different concurrency requirements:
1. **`free`**: Called when entities are destroyed, requires mutable access (ensuring no concurrent allocations)
2. **`alloc`**: Normal allocation from the main thread, can assume no concurrent `free` operations
3. **`remote_alloc`**: Allocation from any thread at any time, must handle concurrent `free` operations

The most complex part of the implementation is the free list - a data structure that tracks which entity indices are available for reuse. The free list must be:
- Accessible without mutable references
- Resizable (to accommodate varying numbers of freed entities)
- Memory-pinned (addresses of freed entities must remain valid)
- Concurrently safe for both allocation and freeing operations

The implementation uses a split buffer approach similar to `SplitVec`, where the buffer is divided into chunks of exponentially increasing sizes. Each chunk's memory remains pinned once allocated, and new chunks are added as needed. This avoids copying elements during resizing while maintaining stable memory addresses.

```rust
struct FreeBuffer([Chunk; Self::NUM_CHUNKS as usize]);

struct Chunk {
    first: AtomicPtr<Slot>,
}

struct Slot {
    inner: SyncUnsafeCell<Entity>,
}
```

The free list's state is tracked using a single 64-bit atomic integer (`FreeCount`) that packs three pieces of information:
- 33 bits for the length (using a signed representation to handle underflow)
- 1 bit as a disabling flag (acts as a lightweight mutex)
- 30 bits for a generation counter (distinguishes different states of the same length)

This bit-packing allows atomic updates to both the length and generation together, which is crucial for correct concurrent operations.

```rust
struct FreeCountState(u64);

const DISABLING_BIT: u64 = 1 << 33;
const LENGTH_MASK: u64 = (1 << 32) | u32::MAX as u64;
const LENGTH_0: u64 = 1 << 32;
const GENERATION_LEAST_BIT: u64 = 1 << 34;
```

The `remote_alloc` method demonstrates careful concurrent programming. It uses a compare-and-exchange loop to safely allocate from the free list while handling concurrent `free` operations. When a `free` operation is detected (via the disabling flag), the method spins briefly to avoid wasting CPU cycles, yielding the thread after 64 attempts to allow the freeing thread to make progress.

```rust
fn remote_alloc(&self) -> Option<Entity> {
    #[cfg(feature = "std")]
    let mut attempts = 1u32;
    let mut state = self.len.state(Ordering::Acquire);
    loop {
        if state.is_disabled() {
            #[cfg(feature = "std")]
            {
                attempts += 1;
                if attempts.is_multiple_of(64) {
                    std::thread::yield_now();
                } else {
                    core::hint::spin_loop();
                }
            }
            #[cfg(not(feature = "std"))]
            core::hint::spin_loop();
            
            state = self.len.state(Ordering::Acquire);
            continue;
        }
        
        let len = state.length();
        let index = len.checked_sub(1)?;
        let entity = unsafe { self.buffer.get(index) };
        
        let ideal_state = state.pop(1);
        match self.len.try_set_state(state, ideal_state, Ordering::Relaxed, Ordering::Acquire) {
            Ok(_) => return Some(entity),
            Err(new_state) => state = new_state,
        }
    }
}
```

The public API changes are